<!DOCTYPE HTML>

<html>
	<head>
		<title>Printer Reviews for Customer Insights</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Peace's Portfolio</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Projects</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/peacetay/" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
							<li><a href="https://github.com/peacetay" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								</header>
								<div class="image main"><img src="images/printer.jpg" alt="" /></div>
								<h2>Printer Reviews for Customer Insights</h2>
								<p>In the age of online marketplaces, the overwhelming volume of product reviews provides a goldmine of information on customer insights and preferences. 
									Manufacturers can make use of text analytics to extract valuable insights on product strengths, comparisons against competitors and areas for improvement to benefit themselves and consumers.</p>
								<p>This study uses sentiment analysis and topic modelling on reviews of 3 major printer brands (HP, Epson and Canon) on Amazon to extract valuable insights with the goal of helping HP regain its status as a market leader in printer products.</p>
								<p>For this project, we assume the role of consultants hired by HP to do a comparative analysis to extract customer insights about HP’s printer products against its competitors to address the recent erosion of market share (IDC, 2023). Through the unsupervised and supervised techniques, we aim to understand features that customers value (e.g. print quality, customer service) and identify differences in points of satisfaction or dissatisfaction between HP and its competitors to find areas for improvement.</p>								

								<a href="https://github.com/peacetay/PrinterReviews" class="button">View Scripts Here!</a>					
								<p> <br></p>
								<h3>1. Data Sources</h3>
								<p>We have crawled Amazon for customer reviews for a total of 28 printers. 
									This dataset comprises 15 HP ink printers and 13 Canon/Epson printers which are selected by HP stakeholders. 
									The reviews collected were primarily from January 1, 2020, through September 10, 2023. 
									Within this dataset, each product has between 100 to 500 reviews and encompasses ratings spanning from 1 to 5 stars. 
									On top of the review text, the dataset captures other variables described in 0.</p>
								<table>
									<thead>
										<tr>
										<th style = "width:40%">Variable</th>
										<th style = "width:60%">Description</th>
										</tr>
									</thead>
									<tbody>
										<tr>
										<td>Review Date</td>
										<td>The date when customer left the review</td>
										</tr>
										<tr>
										<td>Review Name / Title / Content</td>
										<td>Detailed review information</td>
										</tr>
										<tr>
										<td>Verified Purchase</td>
										<td>The presence of a verified purchase or an empty field indicates review authenticity</td>
										</tr>
										<tr>
										<td>People_find_helpful</td>
										<td>Whether the review is VINE VOICE or not
											(Amazon Vine Voice is a special program where trusted reviewers get early access to free new products for writing review)</td>
										</tr>
										<tr>
										<td>Price / Rating Count / Overall Rating</td>
										<td>Pricing/ Total rating count / average rating information for each product</td>
										</tr>
										<tr>
										<td>Review Model</td>
										<td>Brand and model of the reviewed printer</td>
										</tr>
									</tbody>
								</table>				
								<p>Table 1: List of Data Sources</p>

								<h3>2. Methodology</h3>
								<h4>Procedure & Solution Details</h4>
								<div class="image main"><img src="images/printer-01.jpg" alt="" /></div>
								<p>Figure 1: Solution Approach</p>
								<p>First, we define the problem and set goals to guide the study, then collect review data. Text Pre-Processing is required to get better outputs from subsequent analysis steps. 
									Exploratory Data Analysis uncovers initial trends. 
									Topic Modelling is performed to summarise aspects of customer concerns, followed by building a Sentiment Analysis model for understanding customer sentiments in unlabelled texts. 
									Lastly, we built a Document Retrieval interface for HP's internal stakeholders to easily view individual reviews that match their specific needs. 
									This systematic approach (Figure 1) enables a thorough understanding of customer feedback and shapes informed decisions for product development and customer satisfaction enhancement.</p>
								
								<h3>3. Data Preparation & Exploratory Data Analysis</h3>
								<h4>Data Preparation</h4>
								<p>The following pre-processing steps were performed: lower casing, punctuation removal, stop words removal, spelling correction and Porter stemming. 
									In addition, the 10 most common words and words that only appear once in the corpus were removed since these are not informative. The pyspellchecker package was used to correct common spelling mistakes. 
									Keywords specific to the corpus (e.g. brand and model names) were added to ensure these were not changed. </p>
								
								<h4>Exploratory Data Analysis</h4>
								<p>All printer models had positive average overall ratings above 4.0 stars. 
									Reviews tended towards extremes with higher proportion of reviewers giving 1- or 5-star reviews (Figure 4 1). 
									In addition, negative reviews tended to be longer than positive reviews (Figure 4 2); dissatisfied customers tended to give detailed explanations of problems encountered. 
									As seen in Figure 4 3, Epson's reviews were the most dichotomized with higher proportions of both 1 and 5-star reviews. 
									These contrasting evaluations underscore the challenges of catering to diverse customer needs, where not every product can meet the unique needs and expectations of every consumer. 
									These findings underscore the importance of addressing negative feedback and continuously striving for product improvement in a competitive marketplace. 
									In the realm of popular vocabulary analysis, terms such as “work”, “time”, “connect” and “cartridge” emerged as frequent favorites, echoing the key aspects consumers prioritize when evaluating printers (Figure 4 4). 
									This intriguing landscape of discoveries sets the stage for our subsequent experiments, where we delve deeper into the intricacies of topic modelling and sentiment analysis. 
									These advanced techniques are poised to unveil even more insights into the scenarios we are about to explore, providing a robust foundation for shaping product development and elevating customer satisfaction through data-driven decision-making.</p>
								<div class="image main"><img src="images/printer-02.jpg" alt="" /></div>
								<p>Figure 2: Review Rating Distribution</p>
								<div class="image main"><img src="images/printer-03.jpg" alt="" /></div>
								<p>Figure 3: Average Number of Tokenised Words by Rating</p>
								<div class="image main"><img src="images/printer-04.jpg" alt="" /></div>
								<p>Figure 4: Percentage of Reviews (by Brand and Rating)</p>
								<div class="image main"><img src="images/printer-05.jpg" alt="" /></div>
								<p>Figure 5: Overall Review Vocabulary</p>

								<h3>4. Experiments</h3>
								<h5>Topic Modelling</h5>
								<p>Latent Dirichlet Allocation (LDA) was used to perform topic modelling. 
									A new list of stop words was derived through trail-and-error to remove words that did not contribute to the meaning of the topics, especially subjective words like “love” or “horrible”. 
									Some common words previously removed were added back in for added clarity (e.g. “ink”).</p>
								<p>We attempted to use Umass coherence score to determine the optimum number of topics because it is based on co-occurrence statistics within the corpus rather than an external reference corpus (Mimno et al., 2011). 
									However, topics derived from optimal Umass were not necessarily more coherent, so human evaluation was eventually used to decide the number of topics and hyperparameters of alpha and eta. 
									The final model used alpha=5, eta=0.1 and k=5.</p>
								<h5>Sentiment Analysis</h5>
								<p>We attempted sentiment analysis with TextBlob and VADER (Valence Aware Dictionary and sEntiment Reasoner), which are lexicon- and rule-based methods. 
									Both methods output a score indicating polarity of a review with a positive score meaning a more positive sentiment and a negative score meaning that the review is likely to be negative. 
									A score that is close to or equal to zero would mean the sentiment is neutral. 
									Review ratings are used as the ground truth to determine whether a review is positive, neutral, or negative to compare accuracy of each method. </p>

								<h4>5. Results and Analyses</h4>
								<h5>Topic Modelling</h5>
								<p>In this part of the project, we explore the topics discussed in product reviews through LDA. 
									We further dissect these topics by brand and rating, allowing us to uncover aspects for improvement or positive features highlighted by customers. 
									The 5 topics identified are as follows (see Figure 6 for word clouds): [Topic 0] Print issues, [Topic 1] Support and returns, [Topic 2] Print and scan quality, [Topic 3] Setup and connecting, [Topic 4] Cartridge replacement.</p>
								<div class="image main"><img src="images/printer-06.jpg" alt="" /></div>
								<p>Figure 6: Word Cloud Topics</p>

								<h6>Overall</h6>
								<p>The dominant topic of a document is the topic with the highest probability in the document’s topic probability distribution. 
									Overall, the dominant topics are evenly distributed across the 5 topics (see Figure B 2). </p>
								<div class="image main"><img src="images/printer-07.jpg" alt="" /></div>
								<p>Figure 7: Overall Topic Distribution</p>

								<h6>By Rating and Brand</h6>
								<p>The topics were closely related to rating (see Figure 8). 1- and 2-star reviews frequently mentioned “print issues” and “support and returns”. 
									These two topics are inherently negative as they respectively indicate a failure in the product’s core function and the problems faced by the customer. 
									On the other hand, “print and scan quality” was more commonly associated with positive reviews. </p>
								<p>The “support and returns” topic alone does not tell us what problems drove the customer to seek technical support or return the product, so we found the second-most dominant topics for negative reviews with “support and returns” as the dominant topic (Figure 9). 
									“Print issues” were less prominent for HP’s products, but HP had the highest proportion of “setup and connection” issues among the 3 brands. 
									This could indicate a larger issue where customers could not even set it up to have print issues subsequently. “Cartridge replacement” was also a common concern that customers sought technical support for. 
									Their ink subscription programme “HP Instant Ink” may not be user friendly enough.</p>
								<div class="image main"><img src="images/printer-08.jpg" alt="" /></div>
								<p>Figure 8: Overall Distribution of Dominant Topics by Rating</p>
								<div class="image main"><img src="images/printer-09.jpg" alt="" /></div>
								<p>Figure 9: 2nd Dominant Topic Distribution for "Support and Returns"</p>

								<p>There was a significant difference in the topic distribution across brands (see Figure 10). 
									The dominant topic was equally distributed among the 5 topics for HP. 
									Along with HP’s wide range of products across different price points, we can infer that HP’s strategy is to be an all-rounder that meets different needs. 
									Epson and Canon had an uneven distribution topic. From the high proportion of good reviews about print quality and higher priced products, Epson is likely the market in higher-end printers. Canon had a high proportion of “setup and connecting” and low proportion of “cartridge replacement”. 
									Based on the price point of their products, they likely target budget-conscious buyers.</p>
								<p>HP’s strategy as an all-rounder has served well in the past to get a large customer base. However, to regain its position as the market leader, HP may need to differentiate its product more from its competitors. 
									HP had the highest proportion of positive reviews about “cartridge replacement” (Figure 11), indicating that HP may have an advantage in the ink subscription space. 
									This could be a differentiating point over its competitors. Given that it was also often a point of contention for unhappy customers, HP will need to improve the use¬¬r experience for it to boost sales.</p>

								<div class="image main"><img src="images/printer-10.jpg" alt="" /></div>
								<p>Figure 10: Overall Distribution of Dominant Topics by Brand</p>								
								<div class="image main"><img src="images/printer-11.jpg" alt="" /></div>
								<p>Figure 11: Dominant Topics in Positive Reviews by Brand</p>

								<h5>Sentiment Analysis</h5>
								<p>In this phase, we categorized sentiments into positive, neutral, and negative groups by using compound score to discern and quantify the emotional tone within reviews. 
									The trained model is intended to be applied to text data from other sources that do not have sentiment labels (e.g. social media comments, HP’s customer feedback channels etc). </p>
								
								<h6>Model Accuracy</h6>
								<p>Overall, the lexicon and rule-based model VADER was slightly more accurate than the lexicon-based TextBlob method. 
									The increased accuracy is likely due to VADER’s rules to identify negation and contrastive words (Hutto & Gilbert, 2014). 
									F1 scores for positive reviews were higher than for negative or neutral reviews in both models (Figure 12), likely because positive reviews tend to be shorter. 
									However, the overall accuracy of both models was less than ideal, with VADER being slightly higher at 61% compared to TextBlob. 
									This could be because the length of customer reviews is generally long (average of at least 25 words). 
									As the sentiments are generated at word level and subsequently used to compute the compound score of the review, long reviews with mixed sentiments could have resulted in less accurate compound scores. 
									Although the low recall scores for positive reviews showed that many positive reviews were misclassified as negative or neutral, this could be acceptable as we are more concerned about points of dissatisfaction. 
									As noted earlier, reviewers tend to be more vocal about negative opinions so there may be mismatch when the rating considers positive points not mentioned in the text (Maks & Vossen, 2013). 
									The VADER model was used to further analyse sentiments across topics as it was slightly more accurate.</p>
								<div class="image main"><img src="images/printer-12.jpg" alt="" /></div>
								<p>Figure 12: Classification Reports</p>

								<h6>Overall Sentiments</h6>
								<p>On the whole, Epson had the highest percentage of positive sentiments, followed very closely by HP and then Canon. 
									The order was reversed for negative sentiments with Canon as the highest and Epson as the lowest (Figure 13). Epson has the most favourable customer sentiments. 
									A deeper analysis of what Epson performed well in could shed light on areas of improvement for HP to remain competitive. </p>
								<div class="image main"><img src="images/printer-13.jpg" alt="" /></div>
								<p>Figure 13: Percentage of Sentiments across Brands</p>								
								
								<h6>Sentiments by Topic and Brand</h6>
								<p>Using the topics derived from LDA, we analysed sentiment distribution at a topic and brand level. The findings were similar to those found using ratings in Section 6.1 above. “Support and returns” was the most prevalent dominant topic for negative reviews due to the negative connotations of the topic, and “print and scan quality” was the most prevalent dominant topic for positive reviews (Figure 14). 
									Figure 15 shows the topic distribution by sentiment and brand. For HP’s positive reviews, “print and scan quality” is the highest followed closely by “setup and connecting” and “cartridge replacement”. 
									Again, this suggests that HP is an all-rounder, and positive sentiments are mostly equally distributed among the 3 features. However, this could also mean that HP does not have a unique selling point to make customers choose their products over their competitors’, which the marketing and product team may want to take into consideration. 
									In comparison, Epson’s positive sentiments are strongly associated with “print and scan quality” which is clearly the standout feature for Epson printers. 
									Given that printing and scanning are core features of printers, the respective teams within HP should consider diving deeper why customers are much more satisfied with their competition’s print and scan quality.</p>
								<div class="image main"><img src="images/printer-14.jpg" alt="" /></div>
								<p>Figure 14: Overall Topic Distribution by Sentiments</p>
								<div class="image main"><img src="images/printer-15.jpg" alt="" /></div>
								<p>Figure 15: Topic Distribution for Each Sentiment by Brand</p>								
								
								<h3>6. Document Retrieval</h3>
								<p>We tested multiple methods for document retrieval such as Vector Space Model using unigrams and bigrams, as well as TensorFlow Pre-trained Sentence Encoder. 
									The bigram Vector Space Model with cosine similarity demonstrated the best performance and processing efficiency. </p>
								
								<h4>Vector Space Model</h4>
								<p>Initially, we preprocessed the reviews by tokenizing them into unigrams and converted each review into a raw term frequency and TF-IDF vector. The same preprocessing steps are applied to queries to calculate the cosine similarity between these queries and the review corpus. The unigram approach performed admirably for certain queries that consist of standalone keywords. However, it struggles with queries that require preserving the word order, such as "low quality." In such cases, the model retrieves results if the reviews contain the individual words "low" and "quality," even if they are from entirely different contexts, like "good quality low price". <br>
									To address this limitation, we incorporated bigrams into our vocabulary. This enhancement enables us to extract more pertinent reviews even though the similarity scores were lower than those of the unigram model. From Figure 16 and Figure 17, we can see that the top results retrieved are more closely aligned with the "low quality" according to user’s searching intention.</p>
								<div class="image main"><img src="images/printer-16.jpg" alt="" /></div>
								<p>Figure 16: Results from Unigram Model for Query - Low Quality</p>
								<div class="image main"><img src="images/printer-17.jpg" alt="" /></div>
								<p>Figure 17: Results from Bigram Model for Query - Low Quality</p>	

								<h4>Pre-Trained Model</h4>
								<p>Cosine similarity in Vector Space Model relies on presence of common terms to measure similarity and retrieve results. 
									Reviews that are semantically related but do not share keywords will yield a low or zero cosine similarity and cannot be retrieved. 
									In Figure 18, unrelated results may surface since it matches based on common words, which may not capture the semantic context in the query “bad connection”.
									To address this constraint, we employed the TensorFlow Universal Sentence Encoder for embedding reviews at sentence level to account for semantically related terms. 
									Based on the description on TensorFlow Hub website (n.d.), the model is pre-trained on a variety of data sources and can embed sentences into high-dimensional vectors to capture the meaning of word sequences. 
									Thus, it should be able to retrieve documents with similar meanings to the query even if there are no shared keywords. 
									In Figure 19, results for “bad connection” are returned even if the keywords do not match exactly but have similar meanings. </p>
								<div class="image main"><img src="images/printer-18.jpg" alt="" /></div>
								<p>Figure 18: Results from Bigram Model for Query - Bad Connection</p>
								<div class="image main"><img src="images/printer-19.jpg" alt="" /></div>
								<p>Figure 19: Results from Universal Sentence Encoder Embedding</p>	

								<p>However, in our primary document retrieval use cases for the project, users can filter review rating then enter keywords like "paper jam" or "ink subscription," to uncover detailed review information. 
									We have confirmed that these keywords are frequently and explicitly mentioned by customers in their reviews. 
									In these cases, the Bigram Vector Space Model has proven to be a suitable and sufficient solution for our requirements. 
									As a result, we have chosen to incorporate the Bigram model into our final interface.</p>
								
								<h4>UI for In-House Sharing</h4>
								<p>To facilitate inter-departmental information sharing, we developed a user-friendly interface (UI) application using Streamlit, (Figure 20). 
									The application not only facilitates document retrieval through open-ended keyword queries but also offers users flexibility to filter documents by various criteria (brand, rating, and price range) to suit their specific needs. 
									Users can select a topic generated through LDA and further input keywords like "ink subscription" to explore related reviews in depth (Figure 21). 
									The interface presents reviews categorized by brand, enabling users to compare the quantity of related reviews for each brand and delve into model details and review content. </p>
								<div class="image main"><img src="images/printer-20.jpg" alt="" /></div>
								<p>Figure 20: UI App of Review Criteria Selection</p>
								<div class="image main"><img src="images/printer-21.jpg" alt="" /></div>
								<p>Figure 21: Results from Document Retrieval UI App</p>	

								<h3>7. Discussion and Gap Analysis</h3>
								<p><b>Topic modelling:</b> Evaluation of meaning is inherently difficult for machines, hence the objective measurements of model quality like coherence score and perplexity are often unreliable. 
									As the corpus for this study is not very large, human evaluation was still possible. However, as the number of topics grows, this will pose as a challenge. In addition, we used trail-and-error to refine the stop words list to get better topic key words that were not merely collections of opinions. This could have been optimized by using sentiment analysis to identify and remove high frequency opinion words. <br>
								<b>Sentiment analysis:</b> The overall accuracy of the sentiment analysis model was less than ideal when compared against the actual review ratings at only 61%. 
								While there may be more inclination for the company to focus on the negative sentiments which had a higher recall of 68%, the model still has room for improvement. After a closer inspection, our team realized that the length of the reviews could be one of the reasons for the poor accuracy where there were mixed sentiments within a review. 
								Some reviews were also not completely relevant to the product where it included opinions that users had about other products to draw comparisons. This may be addressed by performing analysis at a sentence level and using a more sophisticated model that could identify the targets of the sentiments. 
								Variants of LDA that incorporate sentiment analysis (e.g. Joint-Sentiment-Topic) may be able to pick up topics within the sentiments for more detailed understanding of features of printers and the attached sentiments.<br>
								<b>Document retrieval:</b> The accuracy of document retrieval results was enhanced by implementing a two-step process in our interface. Initially, users could select negative or positive ratings, and then input brief, neutral keywords, avoiding the use of subjective terms like "bad" or "good." 
								Unfortunately, the bigram embedding model may sometimes return unrelated reviews with a high cosine similarity score when users input subjective descriptions such as "bad connections". This occurs because the vector space model relies on exact word matches without considering word sequences or semantics. 
								Additionally, our current document retrieval model is primarily designed for short keyword inputs and may not perform optimally when users input sentences. To enhance both the model's performance and the user experience, incorporating a Large Language Model (LLM) for sentence embedding and leveraging cosine similarity calculations would be beneficial. This approach would allow users to input longer query requests and receive semantically relevant results in document retrieval.</p>
								
								<h3>8. Future Work and Conclusion</h3>
								<p>Future work for this project can encompass various avenues for improvement and expansion, including: </p>
								<p>•	<b>Fine-Tuning Models:</b> Refine the sentiment analysis and topic modeling models to achieve higher accuracy and better performance. Experiment with different algorithms and approaches to enhance the quality of insights.<br>
								•	<b>Feedback Loop:</b> Including a review rating system for the business end users to create a feedback loop on the helpfulness and accuracy of the results to improve the performance of document retrieval.<br>
								•	<b>Time-Series & Sales Analysis:</b> Discovery of relationship between sentiment scores and historical sales data would also help to reveal meaningful insights regarding how sentiment impacts product performance.<br>
								•	<b>Integration with e-Commerce Platforms:</b> Create integrations or plugins for e-commerce platforms like Amazon to provide shoppers with instant sentiment analysis and topic summaries for products they are considering.<br>
								•	<b>Integration with Customer Chatbots:</b> Create integrations or plugins to detect sentiments from customer service calls transcript/ conversations to help customer chatbot engage contextually.<br>
								•	<b>Sentiment-Based Product Improvement:</b> Collaborate with manufacturers to use sentiment analysis data for product enhancements and innovations based on customer feedback, thereby creating a feedback loop.</p>
								<p> <br></p>
								<a href="https://github.com/peacetay/ProductReviews" class="button">View Scripts Here!</a>					
							</section>

					</div>

				<!-- Footer -->
				<footer id="footer">
					<section class="split contact">
						<section class="alt">
							<h3>Email</h3>
							<p><a href="peacetay@gmail.com">peacetay@gmail.com</a></p>
						</section>
						<section>
							<h3>Social</h3>
							<ul class="icons alt">
								<li><a href="https://www.linkedin.com/in/peacetay/" class="icon brands alt fa-linkedin"><span class="label">Linkedin</span></a></li>
								<li><a href="https://github.com/peacetay" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
					</section>
				</footer>
		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>